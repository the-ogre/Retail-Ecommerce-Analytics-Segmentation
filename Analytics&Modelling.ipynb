{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fde7bdbe",
      "metadata": {
        "id": "fde7bdbe"
      },
      "source": [
        "# 1. IMPORTING DATA AND LOADING NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655c266c",
      "metadata": {
        "id": "655c266c"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import pandas as pd                                                 \n",
        "pd.set_option('display.max_columns', None)                              \n",
        "pd.set_option('display.max_colwidth', None)                           \n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import numpy as np                                                  \n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use('dark_background')\n",
        "import seaborn as sns                                              \n",
        "sns.set(style='whitegrid')\n",
        "sns.color_palette('dark')\n",
        "%matplotlib inline\n",
        "#------------------------------------------------------------------------------\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import zscore\n",
        "from scipy.spatial import distance\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2794af74",
      "metadata": {
        "id": "2794af74"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('renttherunway.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-fGooqMN_FA9",
      "metadata": {
        "id": "-fGooqMN_FA9"
      },
      "source": [
        "# 2. DATA CLEANSING AND EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "foiKFBtRDkQ_",
      "metadata": {
        "id": "foiKFBtRDkQ_"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JT34jdrcVUyJ",
      "metadata": {
        "id": "JT34jdrcVUyJ"
      },
      "outputs": [],
      "source": [
        "#defining a function\n",
        "def ifDuplicateSamples(data):\n",
        "  NoOfDuplicateRows = data.duplicated().sum()\n",
        "  if NoOfDuplicateRows == 0:\n",
        "    print(\"There are no duplicate rows\")\n",
        "    return\n",
        "  else:\n",
        "    print(\"There are \",NoOfDuplicateRows,\"duplicate rows\")\n",
        "\n",
        "#running it on current data\n",
        "ifDuplicateSamples(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AM4v6VH-WAhs",
      "metadata": {
        "id": "AM4v6VH-WAhs"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1b64fb",
      "metadata": {
        "id": "ce1b64fb"
      },
      "source": [
        "- Let's remove the unnecessary columns like text etc. I could have done sentiment analysis on them to extract something useful, but I will limit the scope of this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5a20a5",
      "metadata": {
        "id": "be5a20a5"
      },
      "outputs": [],
      "source": [
        "df.drop(columns= ['Unnamed: 0','review_date', 'review_text','review_summary'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f5e080",
      "metadata": {
        "id": "a4f5e080"
      },
      "outputs": [],
      "source": [
        "#checking the unique users and items\n",
        "print(\"Number of unique users: {user:,}\".format(user=df[\"user_id\"].nunique()))\n",
        "print(\"Number of unique items: {item:,}\".format(item=df[\"item_id\"].nunique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c44dc2",
      "metadata": {
        "id": "72c44dc2"
      },
      "source": [
        "- There are in total 192544 samples (transactions), out of them, 105,571 are unique users. \n",
        "- So on an average, one user makes two transactions. But we can be a little more specific and see how many make only one transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c4cb0f8",
      "metadata": {
        "scrolled": true,
        "id": "6c4cb0f8"
      },
      "outputs": [],
      "source": [
        "df['user_id'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5499b9f",
      "metadata": {
        "id": "a5499b9f"
      },
      "source": [
        "- Customer with user_id '691468' has made 436 transactions. \n",
        "- There are many such users with unexpectedly high number of transactions. \n",
        "- Let us dig a little deeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9e368a",
      "metadata": {
        "scrolled": true,
        "id": "af9e368a"
      },
      "outputs": [],
      "source": [
        "df[df['user_id'] == 691468].describe(include='all').T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1743fa63",
      "metadata": {
        "id": "1743fa63"
      },
      "source": [
        "- When it comes to personal bodily attributes like weight, height etc, there are no changes. They are all same, as expected. But the other details defffer based on the product she has rented for.\n",
        "IMPORTANT ASSUMPTION:\n",
        "- I am going to make an important assumption now. I will \n",
        "- I am majorly involved with customer segmentation. I have data for every customer, but there are many customers who make more that (some ranging from 200 to 400) transactions. I will not aggregate them. \n",
        "- I will take each transaction as a separate customer. This is an important ASSUMPTION that needs to be enumerated beforehand. \n",
        "- Keeping this is mind, I am going to remove attributes like 'user_id' and'item_id' which do not offer much to a sinlge transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SvJqFJaTEPMs",
      "metadata": {
        "id": "SvJqFJaTEPMs"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['user_id','item_id'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BlCotniBZIiy",
      "metadata": {
        "id": "BlCotniBZIiy"
      },
      "source": [
        "- Let's rename a few feature names for for ease-of-access in pandas.\n",
        "- I also make sure that bust size is renamed as 'bra_size' which is what the column values stand for. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w4lXhl29EUcO",
      "metadata": {
        "id": "w4lXhl29EUcO"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={'bust size':'bra_size','rented for':'rented_for','body type':'body_type'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103c762e",
      "metadata": {
        "id": "103c762e"
      },
      "source": [
        "### Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2CP9tlU_EZOV",
      "metadata": {
        "id": "2CP9tlU_EZOV"
      },
      "outputs": [],
      "source": [
        "def missing_data(df):\n",
        "  missing_data = pd.DataFrame({'net_missing': df.isnull().sum(), '%missing': (df.isnull().sum()/len(df))*100})\n",
        "  print(missing_data)\n",
        "\n",
        "missing_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wM_gAPP-FgJ1",
      "metadata": {
        "id": "wM_gAPP-FgJ1"
      },
      "source": [
        "- There are ample number of missing values in our data. \n",
        "- We need to do a little EDA to get a sense of the data before we can go ahead with missing value inputation. \n",
        "- An idea of outliers will also be helpful. Let us observe the data manually first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ghp8h12mQCVX",
      "metadata": {
        "id": "Ghp8h12mQCVX"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vrVwLJ3tQHgX",
      "metadata": {
        "id": "vrVwLJ3tQHgX"
      },
      "source": [
        "- There are three numerical features. Let's create a list for them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jdXXnXC3QdR_",
      "metadata": {
        "id": "jdXXnXC3QdR_"
      },
      "source": [
        "**rating**\n",
        "- rating goes from 2 to 10. \n",
        "- It has a few mising values. We will impute them based on data. \n",
        "- I would love to choose them based on the review text, but depends. Not many missing values. \n",
        "- I don't think there are outliers.\n",
        "\n",
        "**size**\n",
        "- It is the standardized size of the product\n",
        "- size ranges from 0 to 58 \n",
        "- mean is almost equal to median, signifying a normal distribution. \n",
        "- It was chosen by the customer, but labelled by the company.\n",
        "- As it's a rent company, customers don't have the freedom to 'change' their size. And they can only 'choose.\n",
        "- So, all sizes are valid. This column looks consistent, as it should. \n",
        "- But we will still check for outliers.\n",
        "\n",
        "**age**\n",
        "- age ranges from 0 to 117, which is obviously unreasonable.\n",
        "- I will first check the customers who have put their age less than 15, and then those who have over 90, and check other values.\n",
        "- It is reasonable to assume that some customers are private and they do not wish to share their details. But it still doesn't make sense for one to put their age as 117.\n",
        "\n",
        "\n",
        "*Let me analyze the data feature by feature.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f57c2e",
      "metadata": {
        "id": "81f57c2e"
      },
      "source": [
        "### age"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDkf2KPnV_dp",
      "metadata": {
        "id": "tDkf2KPnV_dp"
      },
      "source": [
        "- First I will deal with 'age'\n",
        "- Clearly there are too many outliers in this column, as I has expected. Let's first check the customers who put their age over 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278eafa6",
      "metadata": {
        "id": "278eafa6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "sns.boxplot(x=df['age'])\n",
        "plt.title(\"Boxplot of age\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c14cf6",
      "metadata": {
        "id": "c9c14cf6"
      },
      "source": [
        "- age ranges from 0 to 116, which is clearly bogus.\n",
        "- I will first check for ages over 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c6e017",
      "metadata": {
        "id": "a1c6e017"
      },
      "outputs": [],
      "source": [
        "df.groupby('age').rating.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xu7ETi-3X8Iy",
      "metadata": {
        "id": "xu7ETi-3X8Iy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df[df.age >= 100].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3DnNEc8LYWeC",
      "metadata": {
        "id": "3DnNEc8LYWeC"
      },
      "source": [
        "- What is funny about this subset is that almost all the ratings are either 10 or 8. I think that these are fakes entries put in order to better the ratings.\n",
        "- I will remove all these rows from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GH44Bx9AYvwq",
      "metadata": {
        "id": "GH44Bx9AYvwq"
      },
      "outputs": [],
      "source": [
        "df.drop(df[df['age'] >= 100].index, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ttpDnPDxZFGa",
      "metadata": {
        "id": "ttpDnPDxZFGa"
      },
      "source": [
        "- Let's check for the subset of ages over 80 also, just to be sure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jXY5EAo6ZEey",
      "metadata": {
        "id": "jXY5EAo6ZEey"
      },
      "outputs": [],
      "source": [
        "df[df.age >= 65].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QOyloRRQZhwz",
      "metadata": {
        "id": "QOyloRRQZhwz"
      },
      "source": [
        "- Mean is 9.14, which is around the mean of the whole dataset.\n",
        "- So we won't do much. Though it's quite abrasive to think that customers older than 65 would even care to rent outfits from an online store considering the demography at hand.\n",
        "- Let's check a few samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f983e65",
      "metadata": {
        "id": "9f983e65"
      },
      "outputs": [],
      "source": [
        "df[df.age >= 65].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b0988a",
      "metadata": {
        "id": "96b0988a"
      },
      "source": [
        "- To me, the data looks incongruous to my sensibilities. Almost all of them have given a rating of 10\n",
        "- Also, there are not many samples belonging to this subset. I will remove them without any guilt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8a44d9",
      "metadata": {
        "id": "7e8a44d9"
      },
      "outputs": [],
      "source": [
        "df.drop(df[df['age'] >= 65].index, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAvX1ExScApC",
      "metadata": {
        "id": "XAvX1ExScApC"
      },
      "source": [
        "- Now let's check for ages less than 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139bcae7",
      "metadata": {
        "id": "139bcae7"
      },
      "outputs": [],
      "source": [
        "df[df.age <= 10].sample(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kBwcdQfMcXTz",
      "metadata": {
        "id": "kBwcdQfMcXTz",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df[df.age <= 10].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8DtrCW2Bchsz",
      "metadata": {
        "id": "8DtrCW2Bchsz"
      },
      "source": [
        "- Little kids are not supposed to have 'bust_size' of 32 or 38\n",
        "- Almost all are rated 10\n",
        "- Should be removed.\n",
        "- Lets' check for age 10 to 15. Ideally we should remove them before looking because the company has options only for girls characterize as teens, which is over 15 if we go extreme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559c3ffd",
      "metadata": {
        "id": "559c3ffd"
      },
      "outputs": [],
      "source": [
        "df[(df.age >= 10) & (df.age <= 16)].sample(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lB0yLDQrdhrb",
      "metadata": {
        "id": "lB0yLDQrdhrb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df[(df.age >= 10) & (df.age <= 16)].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-stHZ6yyeWnM",
      "metadata": {
        "id": "-stHZ6yyeWnM"
      },
      "source": [
        "- Similar trend of suspiciously high ratings. I will remove them.\n",
        "- Also, not many samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c_sNSbqzedoL",
      "metadata": {
        "id": "c_sNSbqzedoL"
      },
      "outputs": [],
      "source": [
        "df.drop(df[df['age'] <= 16].index, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aE4sV9eGeliL",
      "metadata": {
        "id": "aE4sV9eGeliL"
      },
      "source": [
        "- Let's be sure, and check the data between 15 to 18 once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ncbenfu0etSa",
      "metadata": {
        "id": "Ncbenfu0etSa"
      },
      "outputs": [],
      "source": [
        "df[(df.age > 16) & (df.age <= 18)].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bDklqymfb3L",
      "metadata": {
        "id": "5bDklqymfb3L"
      },
      "source": [
        "- Customers of this age segment have given a mean rating of 9.3, which is still considerable. But I will keep them.\n",
        "- Fake customers can be attacked in a different way. Let's check the boxplot before moving away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add0aff2",
      "metadata": {
        "id": "add0aff2"
      },
      "outputs": [],
      "source": [
        "df['age'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedddbe1",
      "metadata": {
        "scrolled": true,
        "id": "fedddbe1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "sns.boxplot(x=df['age'])\n",
        "plt.title(\"Boxplot of age after first outlier treatment\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e733eb",
      "metadata": {
        "id": "e4e733eb"
      },
      "source": [
        "- There are a few outliers. But they pertain to the uniqueness of data, as it tries to capture the entire population, females in focus.\n",
        "- Let's do a log transformation, before going ahead. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6dc2727",
      "metadata": {
        "id": "e6dc2727"
      },
      "source": [
        "### size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823734eb",
      "metadata": {
        "id": "823734eb"
      },
      "source": [
        "- Now, let's see 'size' once again. It is the 'standardized size of the product' according to the data dictionary. So clearly, it is something printed on the products and catalogued, and filled into database based on the product rented by a particular customer.\n",
        "- In America, where the company is located, the sizes range from 0 to 22, with no odd values. That is also the case on the website of renttherunway.\n",
        "- But we observed that size values range upto 58 in this dataset. This is quite confusing. \n",
        "- So I check on its countplot to check the counts of possible sizes in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eWt6v_7efuUu",
      "metadata": {
        "id": "eWt6v_7efuUu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,7))\n",
        "sns.countplot(data=df,x='size', palette='pastel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vFMDUZxentY0",
      "metadata": {
        "id": "vFMDUZxentY0"
      },
      "source": [
        "- Too many customers have sizes as odd. So, either the data has already been processed, and the values have been given a different range or there is something inherent wrong about it. \n",
        "- The odd values cannot be explained even if consider that the sizes are a mix of American and European size possible value of dress size.\n",
        "- Let me still have a glance of the customers who have a size over 28."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9eB2nopoNl1",
      "metadata": {
        "id": "D9eB2nopoNl1"
      },
      "outputs": [],
      "source": [
        "df[df['size']>28].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xUMxWEqopV-",
      "metadata": {
        "id": "2xUMxWEqopV-"
      },
      "source": [
        "- There are over 6000 customers who have size greater than 28. A significant value.\n",
        "- Most of them have rated highly (mean=9)\n",
        "- age as well as size is normally distributed.\n",
        "- I don't think I am going to do anything about it. It requires intervention of an expert who can explain what these values mean.\n",
        "- For now, I am going to take this as it is. \n",
        "- I may decide to drop it after bivariate analysis, because size of the product is a property of the product. It is directly related to the customer through their body size (as in height, weight, body-type, and bra-size).\n",
        "- I will explore these relationships and decide on this feature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65806272",
      "metadata": {
        "id": "65806272"
      },
      "source": [
        "### rating"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad71facf",
      "metadata": {
        "id": "ad71facf"
      },
      "source": [
        "- Let me convert rating scale to 1-5: There are only even numbers (i.e. 2, 4, 6, 8, 10) so condensing it to this scale seems reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bD94ow7QP_Km",
      "metadata": {
        "id": "bD94ow7QP_Km"
      },
      "outputs": [],
      "source": [
        "df[['age','rating']].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BAjdClMwNsqW",
      "metadata": {
        "id": "BAjdClMwNsqW"
      },
      "source": [
        "- Before going further, I will impute the age and rating by their median (which are almost same as mean) because I wish to preserver their integral integrity.\n",
        "- Median is appropriate value to be filled in place of nulls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ak2u94aHPioO",
      "metadata": {
        "id": "ak2u94aHPioO"
      },
      "outputs": [],
      "source": [
        "df['age'].fillna(df['age'].median(), inplace=True)\n",
        "df['rating'].fillna(df['rating'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8003f78",
      "metadata": {
        "id": "c8003f78"
      },
      "source": [
        "### height and weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QcGjNNHXrhfg",
      "metadata": {
        "id": "QcGjNNHXrhfg"
      },
      "source": [
        "- Let's focus on weight and height of the customers.\n",
        "- First we will remove the lbs part from the weight feature values, keeping in mind that around 15% of them are missing.\n",
        "- The we will convert the height in ft-inch to inch, keeping the values in inches for it's an American dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DtxFQffzsobn",
      "metadata": {
        "id": "DtxFQffzsobn"
      },
      "outputs": [],
      "source": [
        "# In the following command, I have extracted all the numerics (0-9) using extract method of string object\n",
        "#we also convert it into numeric by using pandas' to_numeric method\n",
        "df[\"weight\"] = df[\"weight\"].str.extract(\"([0-9]+)\", expand=True).apply(pd.to_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "anC4MOXKvRj_",
      "metadata": {
        "id": "anC4MOXKvRj_"
      },
      "outputs": [],
      "source": [
        "# function to convert height into inches\n",
        "# let height stand for the feature-column of the variable height in the dataframe\n",
        "def to_inch(height):\n",
        "  #extracting the numerical strings from the height values (of the form FT'IN'')\n",
        "  height = height.str.extractall(\"([0-9]+)\").reset_index()\n",
        "  #the above function removes the ' and '' from the values and saves the two remaining numbers in height Series\n",
        "  #creating two  new Series objects by extracting the fiest and second values stored in height Series\n",
        "  # we use boolean filter using index values to achieve this. \n",
        "  feet = (height[\"match\"] == 0)\n",
        "  inch = (height[\"match\"] == 1)\n",
        "  #converting the feet and inch into numeric and multiplying feet by 12\n",
        "  feet_changed = height[feet].drop([\"level_0\", \"match\"], axis=1).reset_index(drop=True).apply(pd.to_numeric) * 12\n",
        "  inch_changed = height[inch].drop(['level_0','match'], axis=1).reset_index(drop=True).apply(pd.to_numeric)\n",
        "  #adding the converted featrures and return them \n",
        "  return feet_changed + inch_changed\n",
        "\n",
        "#running the function on the height column\n",
        "df['height']=to_inch(df['height'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3U_AvrXDuBL",
      "metadata": {
        "id": "I3U_AvrXDuBL"
      },
      "source": [
        "- Now let us check the height and weight values to see if they make sense.\n",
        "- First we check their boxplots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-eh6FkndD1Uz",
      "metadata": {
        "id": "-eh6FkndD1Uz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "df[['height','weight']].boxplot()\n",
        "plt.title(\"Boxplots of Height and Weight\", fontsize=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mpLa4afkETI-",
      "metadata": {
        "id": "mpLa4afkETI-"
      },
      "outputs": [],
      "source": [
        "df[['height','weight','age']].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2b0637",
      "metadata": {
        "id": "0c2b0637"
      },
      "source": [
        "### weight"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xYJizR11EnJ8",
      "metadata": {
        "id": "xYJizR11EnJ8"
      },
      "source": [
        "- From the boxplots, we felt that there are too many outliers in the weight column. But mean is almost equal to median, pointing towards a usual distribution of weights in a sample of population\n",
        "- The average weight of the customers is 137 pounds, and the weights range from 50 to 300 pounds, with significant people lying in the weight range 135 to 148 (as seen in the third quartile.\n",
        "- Everything looks consistent to me.\n",
        "- And I will impute the null value with median."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ZlE4GhWEm49",
      "metadata": {
        "id": "2ZlE4GhWEm49"
      },
      "outputs": [],
      "source": [
        "df['weight'].fillna(df['weight'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2e166b0",
      "metadata": {
        "id": "a2e166b0"
      },
      "source": [
        "### height"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CqC7YaTEKust",
      "metadata": {
        "id": "CqC7YaTEKust"
      },
      "source": [
        "- height ranges from 54 to 78 inches, which seems reasonable for a sample human population\n",
        "- the mean height of the given set of customers is almost same as median, indicating the presence of a normal distribution.\n",
        "- I will impute the null values with mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H92MOosTMV2z",
      "metadata": {
        "id": "H92MOosTMV2z"
      },
      "outputs": [],
      "source": [
        "df['height'].fillna(df['height'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YLdnrtaWV3gq",
      "metadata": {
        "id": "YLdnrtaWV3gq"
      },
      "source": [
        "- We have seen the numeric features.\n",
        "- Let's focus on the categorical ones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdfadb0",
      "metadata": {
        "id": "ccdfadb0"
      },
      "source": [
        "### bra_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd03e9d",
      "metadata": {
        "id": "1fd03e9d"
      },
      "source": [
        "- First try to understand how many types of values are present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zi7MgZ0eWkET",
      "metadata": {
        "id": "Zi7MgZ0eWkET",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df['bra_size'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cmwiPF5VXRkI",
      "metadata": {
        "id": "cmwiPF5VXRkI"
      },
      "source": [
        "- There are 106 types of bra sizes found in the customer dataset we have\n",
        "- Each bra size, as the nomenclature is, is given by two numeric digits followed by an alphabetic character.\n",
        "- The numeric digits correspond to the band size, and alphabetic characters correspond to the bust/cup size.\n",
        "- Now, looking at the data we have, most of the customers have a band size of 34, which is the case worldwide. So, the data values are consistent.\n",
        "- We can impute the values with mode of the all the possible customer values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fsxSUkJs9hdZ",
      "metadata": {
        "id": "fsxSUkJs9hdZ"
      },
      "outputs": [],
      "source": [
        "df['bra_size'].fillna(df['bra_size'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kdg9eM8e-0l4",
      "metadata": {
        "id": "kdg9eM8e-0l4"
      },
      "source": [
        "- I am going to split this column into two different columns\n",
        "  - There are too many possible classes (106)\n",
        "  - The algorithms I am going to use to cluster are centroid based, and do not perform well when there are too many categorical features.\n",
        "  - For now, let's continue with the other features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1f2eff2",
      "metadata": {
        "id": "c1f2eff2"
      },
      "source": [
        "### body_type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CyK46IaxDee8",
      "metadata": {
        "id": "CyK46IaxDee8"
      },
      "source": [
        "- We can already see that we have to impute missing values in this column with mode.\n",
        "- But let us still be sure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0GGVOaDdwl",
      "metadata": {
        "id": "3b0GGVOaDdwl"
      },
      "outputs": [],
      "source": [
        "df['body_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oFzRQ_vIHVWE",
      "metadata": {
        "id": "oFzRQ_vIHVWE"
      },
      "source": [
        "- There are seven classes. Most of the customers chose to go for a body hugging dress as is clear from their choice of hourglass, athletic or pear. \n",
        "- We will impute the missing values with mode. \n",
        "- After proper multivariate analysis, I might choose to 'reduce' the number of classes by considering one or two classes as same. \n",
        "- Or I might even drop this feature altogether."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NRXdpACeICw-",
      "metadata": {
        "id": "NRXdpACeICw-"
      },
      "outputs": [],
      "source": [
        "df['body_type'].fillna(df['body_type'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RlCth2SYIU7K",
      "metadata": {
        "id": "RlCth2SYIU7K"
      },
      "source": [
        "Let us perform mode imputation with rented_for as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zqpj1x1gIcCG",
      "metadata": {
        "id": "Zqpj1x1gIcCG"
      },
      "outputs": [],
      "source": [
        "df['rented_for'].fillna(df['rented_for'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hIUdemMbIMNJ",
      "metadata": {
        "id": "hIUdemMbIMNJ"
      },
      "outputs": [],
      "source": [
        "missing_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wlqpfn1fIjaL",
      "metadata": {
        "id": "wlqpfn1fIjaL"
      },
      "source": [
        "Thus, missing value imputation has been done."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mtpnreYd-xfR",
      "metadata": {
        "id": "mtpnreYd-xfR"
      },
      "source": [
        "- Now, we are ready to go ahead with EDA, feature engineering and processing.\n",
        "- I will save the data in a new dataframe, and conntinue with the process in a separate page to make the entire thing more readable, and avoid refreshing and computing the values again and again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k0MbGlromjIb",
      "metadata": {
        "id": "k0MbGlromjIb"
      },
      "outputs": [],
      "source": [
        "df1 = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NVGnIwv5cMrB",
      "metadata": {
        "id": "NVGnIwv5cMrB"
      },
      "outputs": [],
      "source": [
        "df.to_csv('cleaned1_renttherunway.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e5c9d6",
      "metadata": {
        "id": "e9e5c9d6"
      },
      "source": [
        "### bra_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "STNqVZd9qdoq",
      "metadata": {
        "id": "STNqVZd9qdoq"
      },
      "source": [
        "- We saw that this feature has too many classes. So clusteing would be a bit difficult for a very complex data.\n",
        "- Also, bra size is actually is summation of two quantities. One is band size, reprensented by the numeral and other is the bust size, represented by the number.\n",
        "- So, we can have two features representing this feature. And that's what I am going to do. I will split this feature.\n",
        "- As in the case of weight, I will extract the numerals and alphabets using a basic regex command 'extract'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2va5aPTvqdCz",
      "metadata": {
        "id": "2va5aPTvqdCz"
      },
      "outputs": [],
      "source": [
        "df1[\"band_size\"] = df1[\"bra_size\"].str.extract(\"([0-9]+)\", expand=True).apply(pd.to_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kMyNjl_iqF1C",
      "metadata": {
        "id": "kMyNjl_iqF1C"
      },
      "outputs": [],
      "source": [
        "df1[\"cup_size\"] = df1[\"bra_size\"].str.extract(\"([a-z]+)\", expand=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6m8Om0EqtTGs",
      "metadata": {
        "id": "6m8Om0EqtTGs"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['bra_size'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e606aef9",
      "metadata": {
        "id": "e606aef9"
      },
      "source": [
        "### band_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766a92ae",
      "metadata": {
        "id": "766a92ae"
      },
      "outputs": [],
      "source": [
        "df1['band_size'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9881bd52",
      "metadata": {
        "id": "9881bd52"
      },
      "source": [
        "- There aren't many possble values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a76f3ca4",
      "metadata": {
        "scrolled": true,
        "id": "a76f3ca4"
      },
      "outputs": [],
      "source": [
        "df1['band_size'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11adae48",
      "metadata": {
        "id": "11adae48"
      },
      "source": [
        "- band_size ranges from 28 to 48\n",
        "- We are good to go as far these values are concerned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520b54e8",
      "metadata": {
        "id": "520b54e8"
      },
      "source": [
        "### cup_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b472ddf",
      "metadata": {
        "scrolled": false,
        "id": "9b472ddf"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f8e62e",
      "metadata": {
        "id": "e5f8e62e"
      },
      "source": [
        "- These data points have been filled by the customers. \n",
        "- So some are 'ddd' while some are 'f' but both mean the same.\n",
        "- A shallow dive into the problem from the perspective of business brings to us the fact fact tha\n",
        "    - dd is e, and ddd is f\n",
        "    - aa is something that comes before a in terms of size.\n",
        "- Let's make some replacements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171f3538",
      "metadata": {
        "id": "171f3538"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].replace({'dd':'e','ddd':'f'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9abe55e",
      "metadata": {
        "id": "a9abe55e"
      },
      "source": [
        "- Now, I will label encode. I know that LabelEncoder() encodes data on the basis of alphabetical ordering. So it will take 'a' before 'aa' by default, which will be out of order. \n",
        "- So, before label encoding I will rename 'a' as 'ab', so encoding goes exactly as I want\n",
        "- After label encoding the symbols will be replaced by the cup size (in inches) represented by them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb323a3",
      "metadata": {
        "id": "6fb323a3"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].replace({'a':'ab'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8b23cd",
      "metadata": {
        "id": "4f8b23cd"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df1['cup_size'] = le.fit_transform(df1['cup_size'])\n",
        "df1['cup_size'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec917af6",
      "metadata": {
        "id": "ec917af6"
      },
      "source": [
        "- From cup_size, we will derive the bust size.\n",
        "- From https://www.macys.com/p/bra-fit-guide/bra-size-fit-faq/, we know that:\n",
        "    - bust_size - band_size = cup_size\n",
        "- We have band and cup size, so we will have bust_size\n",
        "    - bust_size = band_size + cup_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9d3768",
      "metadata": {
        "scrolled": true,
        "id": "1a9d3768"
      },
      "outputs": [],
      "source": [
        "df1['bust_size'] = df1['band_size'] + df1['cup_size']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904acc3b",
      "metadata": {
        "id": "904acc3b"
      },
      "outputs": [],
      "source": [
        "df1['bust_size'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279dd5e3",
      "metadata": {
        "id": "279dd5e3"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['cup_size'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f90ca4e",
      "metadata": {
        "id": "5f90ca4e"
      },
      "source": [
        "- Now, I think that for a woman her bust and band size must have correlation.\n",
        "- Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810fea42",
      "metadata": {
        "id": "810fea42"
      },
      "outputs": [],
      "source": [
        "print(f\"The correlation between band and bust size is: \")\n",
        "print(np.corrcoef(df1['band_size'], df1['bust_size']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b754056",
      "metadata": {
        "id": "6b754056"
      },
      "source": [
        "- This is a very high correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c73ac5a",
      "metadata": {
        "id": "3c73ac5a"
      },
      "outputs": [],
      "source": [
        "sns.regplot(data=df1,x='band_size',y='bust_size')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f82f4dd",
      "metadata": {
        "id": "3f82f4dd"
      },
      "source": [
        "- A very interesting plot indeed.\n",
        "- As expected, for one value of band_size, there are multiple values of bust_size.\n",
        "- To preserve the information contained in both these variables, and do away with correlation, I am going to keep their dot product and save in a new feature called 'chest'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0357a61b",
      "metadata": {
        "id": "0357a61b"
      },
      "outputs": [],
      "source": [
        "df1['chest'] = df1['bust_size'] * df1['band_size']\n",
        "df1.drop(columns=['bust_size','band_size'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "507c19a6",
      "metadata": {
        "scrolled": true,
        "id": "507c19a6"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca33802",
      "metadata": {
        "id": "9ca33802"
      },
      "outputs": [],
      "source": [
        "df1['chest'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wOdKq1f7trkj",
      "metadata": {
        "id": "wOdKq1f7trkj"
      },
      "source": [
        "### 'fit' and 'rating'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yaxICT0nuKbK",
      "metadata": {
        "id": "yaxICT0nuKbK"
      },
      "source": [
        "- fit and rating appear to be connected to each other. A customer is expected to rate a product highly if it fits her well. So, from a practical perspective there is definitely a connection.\n",
        "- let us draw a countplot of rating with fit as hue to visualize the connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874ffbf2",
      "metadata": {
        "id": "874ffbf2"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df1,x='rating',hue='fit')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DsFXs97au6qb",
      "metadata": {
        "id": "DsFXs97au6qb"
      },
      "source": [
        "- As is clear from the above graph, \n",
        "  - customers have rated a product highly if they fit them well \n",
        "  - if the product is small or large, then also, a lot of customers have rated the product highly. \n",
        "  - The count of people rating a product as 1,2 and 3 is significantly small as compared to the count of people rating the product as 4 and 5.  \n",
        "  - So, customers are very specific about their rating if the product is small or large- that is they have rated them lowly, to the extent that one can, from statistical perspective, merge large and small into one subset.\n",
        "  - Let's verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IW97VIrNvkmM",
      "metadata": {
        "id": "IW97VIrNvkmM"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df1.groupby(\"fit\")[\"rating\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XX747CBhxFW2",
      "metadata": {
        "id": "XX747CBhxFW2"
      },
      "source": [
        "- There is little statistical difference between the distribution of customers rating a produst as small or large. \n",
        "- The count of customers rating a product is 1,2,3,4 or 5 are almost same for the classes of small and large. So, we can merge into one as not fit. \n",
        "- So I am converting the feature fit into a binary one. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MxEzJLAc87pT",
      "metadata": {
        "id": "MxEzJLAc87pT"
      },
      "outputs": [],
      "source": [
        "df1[\"fit\"] = np.where((df1[\"fit\"] == 'fit'), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec6beb0",
      "metadata": {
        "id": "bec6beb0"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df1,x='rating',hue='fit')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae9d92b",
      "metadata": {
        "scrolled": false,
        "id": "5ae9d92b"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df1.groupby(\"rating\")[\"fit\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d3df6e",
      "metadata": {
        "id": "c5d3df6e"
      },
      "source": [
        "- The trend continues. It the product fits, the customers are going to rate it highly as in, either 4 or five\n",
        "- The informations contained in the feature 'rating' also contains within itself the necessary information about the customer's choice.\n",
        "- We do not really need the 'fit' feature in the context of customer segmentation.\n",
        "- But to be sure, we will perform a Kruskal Wallis test (as rating is not normally distributed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b82cd43",
      "metadata": {
        "id": "7b82cd43"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kruskal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3d3e31",
      "metadata": {
        "id": "9e3d3e31"
      },
      "outputs": [],
      "source": [
        "stats.kruskal(df1['rating'][df1['fit'] == 0],\\\n",
        "               df1['rating'][df1['fit'] == 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08cf299",
      "metadata": {
        "id": "c08cf299"
      },
      "source": [
        "- A very high value of F statistic and low value of p shows that we have sufficient proof to reject the null hypothesis that the median rating is same for both groups of fit- YES and NO. \n",
        "- We can claim that different rating values mean that fit did not happen.\n",
        "- So the two features - 'rating' and 'fit' are dependent on each other, something we knew intuitively. A person won't rate somethiing highly unless they are sure if the item fits.\n",
        "- So, I can get rid of any one of them. Or keep both of them by saving their dot product in a new vector.\n",
        "- While multiplying the vectors, I will make sure to increment fit values by 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f2b660",
      "metadata": {
        "id": "04f2b660"
      },
      "outputs": [],
      "source": [
        "df1['response'] = df1['rating'] * (df1['fit']+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee153e4",
      "metadata": {
        "id": "aee153e4"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['rating','fit'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UyDhCBL2_q1V",
      "metadata": {
        "id": "UyDhCBL2_q1V"
      },
      "source": [
        "### rented_for"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_BwQ7Kwt_9zX",
      "metadata": {
        "id": "_BwQ7Kwt_9zX",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jPN0QJ56ADTs",
      "metadata": {
        "id": "jPN0QJ56ADTs"
      },
      "source": [
        "- There are 9 classes of this feature, which is a rather huge number. \n",
        "- party and party: cocktail clearly belong to only one class- 'party'. Let's merge them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYfmjEczA3cc",
      "metadata": {
        "id": "wYfmjEczA3cc"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='party: cocktail','rented_for'] = 'party'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kA0bkxzaHPm0",
      "metadata": {
        "id": "kA0bkxzaHPm0"
      },
      "source": [
        "- If a customer (female, let's not forget) goes for a date and chooses 'expensive' rented clothes, she is going to wear similar dress for the ocassions of party. To clarify, I mean to argue that the occasion of party and date can be considered as same, if we focus only on the dress the woman chooses to wear.\n",
        "- So I am reclassifying all the values with occasion as date int0 party."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NZHV0g-HHO-M",
      "metadata": {
        "id": "NZHV0g-HHO-M"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='date','rented_for'] = 'party'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dNYTS_E3OqX9",
      "metadata": {
        "id": "dNYTS_E3OqX9"
      },
      "source": [
        "- Now, work is basically a formal affair.\n",
        "- Someone may choose to go to work in simple tee and jeans, but they won't hire is specially from a company.\n",
        "- So as the customer of renttherunway, a woman is buying for 'formal affair' if she is buying for work. \n",
        "- So I am reclassifying all the 'work' values as formal affair. \n",
        "- I am also renaming 'formal affair' as 'formal'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WTxA9h0bPPcU",
      "metadata": {
        "id": "WTxA9h0bPPcU"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].replace({'formal affair':'formal'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6NNe3oTRPcGG",
      "metadata": {
        "id": "6NNe3oTRPcGG"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='work','rented_for'] = 'formal'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b182f800",
      "metadata": {
        "scrolled": false,
        "id": "b182f800"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea7c5c9",
      "metadata": {
        "id": "2ea7c5c9"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='vacation','rented_for'] = 'everyday'\n",
        "df1.loc[df1.rented_for=='other','rented_for'] = 'everyday'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11eac447",
      "metadata": {
        "id": "11eac447"
      },
      "source": [
        "- Now, I have also categorized 'everyday','other', and 'vacation' into one. \n",
        "- The reason has more to do with statistics than real-life reasoning. \n",
        "- But of course, I have to say that when it comes to dresses, people tend to use similar types of clothes for everyday and vacation - comfortable. \n",
        "- The counts for these classes is also low, signifying a smaller cluster. I would like to see them together. I also want to reduce the sparsity of the data.\n",
        "- The idea is to prepare data for the centroid based algorithms. If I had decided to go for DBSCAN or some other neural based clustering approach, I might have tackled the dataset in a different way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f81817f",
      "metadata": {
        "id": "8f81817f"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BrUQgiWoQLWG",
      "metadata": {
        "id": "BrUQgiWoQLWG"
      },
      "source": [
        "### category"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m2OhmSlcQMxF",
      "metadata": {
        "id": "m2OhmSlcQMxF"
      },
      "source": [
        "- Let's analyze 'category' feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39DvM1cLQPdE",
      "metadata": {
        "id": "39DvM1cLQPdE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df1['category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qKzLLeYOQgN8",
      "metadata": {
        "id": "qKzLLeYOQgN8"
      },
      "source": [
        "- There are 68 classes of category of the item that the customers have the option to buy, considering they have exhausted all of them in this huge dataset.\n",
        "- The range of counts of individual items is significant,\n",
        "- This is going to create problems for the algorithms when they choose to cluster the data, and more so when we are limited ourselves to centroid based algorithms like KMeans and Agglomerative.\n",
        "- A huge percentage of them are 'dress'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7YE-HzaRT7_",
      "metadata": {
        "id": "i7YE-HzaRT7_"
      },
      "outputs": [],
      "source": [
        "dress_percent = len(df1.loc[df1['category']=='dress'])/len(df1.category)*100\n",
        "print(f'The percentage of dress bought by customers is {dress_percent}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h05gLc8dTGI1",
      "metadata": {
        "id": "h05gLc8dTGI1"
      },
      "source": [
        "- In fact, most of the people either rent dress or gown or sheath.\n",
        "    - dress         92560\n",
        "    - gown          44160\n",
        "    - sheath        19227\n",
        "- In total 155947 items belong to either of the above three categories, which is around 3/4th of the total count of transactions. \n",
        "- And it makes sense, as most of the people come here to rent for occasions like wedding or party.\n",
        "- Now, I will club of the classes into one, and call it as 'others'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a749ee13",
      "metadata": {
        "id": "a749ee13"
      },
      "outputs": [],
      "source": [
        "df1['category'] = [x if x in {'dress','gown','sheath'} else 'others' for x in df1['category']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d604fc2",
      "metadata": {
        "scrolled": true,
        "id": "6d604fc2"
      },
      "outputs": [],
      "source": [
        "df1['category'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc3fb98",
      "metadata": {
        "id": "4bc3fb98"
      },
      "source": [
        "### body _type and size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe97ed3",
      "metadata": {
        "id": "dbe97ed3"
      },
      "source": [
        "- let's first see what are the various values in the column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c49e61",
      "metadata": {
        "id": "77c49e61"
      },
      "outputs": [],
      "source": [
        "df1['body_type'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fed4321",
      "metadata": {
        "id": "6fed4321"
      },
      "source": [
        "- body_type has the body type of customer\n",
        "- it is filled by the customer\n",
        "- And intuitively, it is easy to see that it has strong connections with the customer's weight, height, chest and size of the item they choose to rent.\n",
        "- Let's look at them a little closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f404b4b",
      "metadata": {
        "scrolled": true,
        "id": "2f404b4b"
      },
      "outputs": [],
      "source": [
        "df1[['body_type','size','weight','height','chest']].sample(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d9c2f9",
      "metadata": {
        "id": "b4d9c2f9"
      },
      "source": [
        "- First of all, 'size' is quite dubious. We have no idea what measure has been used, as discussed earlier. This feature is either the beyond my scope (as my business understanding is limited) or there is something seriously fishy about the values in this column- something I strongly believe, based on my reading of the subject.\n",
        "- But I do see a sort of correlation it has with weight. Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6ceaaa",
      "metadata": {
        "scrolled": true,
        "id": "5e6ceaaa"
      },
      "outputs": [],
      "source": [
        "sns.regplot(data=df1,x='weight',y='size')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a394f5",
      "metadata": {
        "id": "a4a394f5"
      },
      "source": [
        "- There also seems be a very high correlation between weight and size, which can be substantiated with a corr plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2c5a170",
      "metadata": {
        "scrolled": true,
        "id": "e2c5a170"
      },
      "outputs": [],
      "source": [
        "df1.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbce0f87",
      "metadata": {
        "id": "bbce0f87"
      },
      "source": [
        "- So I am going to drop it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559361b3",
      "metadata": {
        "id": "559361b3"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['size'],inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0342cefc",
      "metadata": {
        "id": "0342cefc"
      },
      "source": [
        "- Now, looking at the feature classes of body_type, there seem to be many interesting names. \n",
        "- That is for marketting purpose. Or perhaps, the data analysis department of renttherunway has a definition of these terms.\n",
        "- As for us, who are trying to feed them into ML algorithms, we cannot be sure about everything except for the relationship body_type PREMPTIVELY has with features like 'height','weight','band_size' and 'cup_size'.\n",
        "- I can so far as to claim that body_type is a polynomial combination of the other features. And this is not just intuition. I see it from the 50 samples. \n",
        "- But, and here is my key argument- these values have been fed by the customers, many of whom were not aware of the temrs either. \n",
        "    - some were in a hurry, so they chose anything.\n",
        "    - some wanted to show off and lied.\n",
        "    - some were confused between apple and pear. \n",
        "- That makes 'body_type' a subjective variable. It has more to do customers' self perception that the reality.\n",
        "- But their BIOLOGICAL features are a better source of their body_type. They are more OBJECTIVE.\n",
        "- So, for the sake of objectivity, I am going to drop this feature. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a70178d",
      "metadata": {
        "id": "1a70178d"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['body_type'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b807ce6",
      "metadata": {
        "id": "8b807ce6"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbG7jvXGVZj2",
      "metadata": {
        "id": "hbG7jvXGVZj2"
      },
      "outputs": [],
      "source": [
        "df2 = df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G3Z13MLAWZXd",
      "metadata": {
        "id": "G3Z13MLAWZXd"
      },
      "outputs": [],
      "source": [
        "df1.to_csv('cleaned2_renttherunway.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba7117",
      "metadata": {
        "id": "b2ba7117"
      },
      "source": [
        "# 3. Data Preparation for model building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m3-9eGkqWQ8N",
      "metadata": {
        "id": "m3-9eGkqWQ8N"
      },
      "source": [
        "- But before that, we have to \n",
        "  - one hot encode, and\n",
        "  - standardize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "In19LNn9cwNm",
      "metadata": {
        "id": "In19LNn9cwNm"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24843ed8",
      "metadata": {
        "id": "24843ed8"
      },
      "outputs": [],
      "source": [
        "to_be_encoded = df2.select_dtypes(include='object').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a14e96",
      "metadata": {
        "id": "04a14e96"
      },
      "outputs": [],
      "source": [
        "for feature in to_be_encoded:\n",
        "  df2[feature] = le.fit_transform(df2[feature])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m5AfbtdXZBPE",
      "metadata": {
        "id": "m5AfbtdXZBPE"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62052d50",
      "metadata": {
        "id": "62052d50"
      },
      "outputs": [],
      "source": [
        "## Standardization\n",
        "scaled_features = StandardScaler().fit_transform(df2.values)\n",
        "df2 = pd.DataFrame(scaled_features, index=df2.index, columns=df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d7074b",
      "metadata": {
        "id": "c0d7074b"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151a6c1d",
      "metadata": {
        "id": "151a6c1d"
      },
      "outputs": [],
      "source": [
        "df2.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jmi_W3rauSAo",
      "metadata": {
        "id": "Jmi_W3rauSAo"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('prepared_renttherunway.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4l0e6kGHaoON",
      "metadata": {
        "id": "4l0e6kGHaoON"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('cleaned1_renttherunway.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18557301",
      "metadata": {
        "id": "18557301"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXBVtqbv4R5a"
      },
      "source": [
        "### bra_size"
      ],
      "id": "BXBVtqbv4R5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O28S6954R5b"
      },
      "source": [
        "- We saw that this feature has too many classes. So clusteing would be a bit difficult for a very complex data.\n",
        "- Also, bra size is actually is summation of two quantities. One is band size, reprensented by the numeral and other is the bust size, represented by the number.\n",
        "- So, we can have two features representing this feature. And that's what I am going to do. I will split this feature.\n",
        "- As in the case of weight, I will extract the numerals and alphabets using a basic regex command 'extract'."
      ],
      "id": "-O28S6954R5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5JyBgXN4R5b"
      },
      "outputs": [],
      "source": [
        "df1[\"band_size\"] = df1[\"bra_size\"].str.extract(\"([0-9]+)\", expand=True).apply(pd.to_numeric)"
      ],
      "id": "g5JyBgXN4R5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV_lzXhi4R5b"
      },
      "outputs": [],
      "source": [
        "df1[\"cup_size\"] = df1[\"bra_size\"].str.extract(\"([a-z]+)\", expand=True)"
      ],
      "id": "WV_lzXhi4R5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZdHBI844R5b"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['bra_size','Unnamed: 0'],inplace=True)"
      ],
      "id": "3ZdHBI844R5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZszo63I4R5c"
      },
      "source": [
        "### band_size"
      ],
      "id": "yZszo63I4R5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tT5gOVV4R5c"
      },
      "outputs": [],
      "source": [
        "df1['band_size'].describe()"
      ],
      "id": "9tT5gOVV4R5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvxYi6gJ4R5c"
      },
      "source": [
        "- There aren't many possble values"
      ],
      "id": "SvxYi6gJ4R5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lPsn54GA4R5d"
      },
      "outputs": [],
      "source": [
        "df1['band_size'].value_counts()"
      ],
      "id": "lPsn54GA4R5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zsdNGiq4R5d"
      },
      "source": [
        "- band_size ranges from 28 to 48\n",
        "- We are good to go as far these values are concerned."
      ],
      "id": "0zsdNGiq4R5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28XF83yf4R5d"
      },
      "source": [
        "### cup_size"
      ],
      "id": "28XF83yf4R5d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "4EDc2eSw4R5d"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].value_counts()"
      ],
      "id": "4EDc2eSw4R5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqpRTi1E4R5e"
      },
      "source": [
        "- These data points have been filled by the customers. \n",
        "- So some are 'ddd' while some are 'f' but both mean the same.\n",
        "- A shallow dive into the problem from the perspective of business brings to us the fact fact tha\n",
        "    - dd is e, and ddd is f\n",
        "    - aa is something that comes before a in terms of size.\n",
        "- Let's make some replacements"
      ],
      "id": "nqpRTi1E4R5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTzMALfe4R5e"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].replace({'dd':'e','ddd':'f'}, inplace=True)"
      ],
      "id": "YTzMALfe4R5e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rR4VhG_4R5e"
      },
      "source": [
        "- Now, I will label encode. I know that LabelEncoder() encodes data on the basis of alphabetical ordering. So it will take 'a' before 'aa' by default, which will be out of order. \n",
        "- So, before label encoding I will rename 'a' as 'ab', so encoding goes exactly as I want\n",
        "- After label encoding the symbols will be replaced by the cup size (in inches) represented by them"
      ],
      "id": "4rR4VhG_4R5e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OXFx9Nu4R5f"
      },
      "outputs": [],
      "source": [
        "df1['cup_size'].replace({'a':'ab'}, inplace=True)"
      ],
      "id": "_OXFx9Nu4R5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNyOT9D4R5f"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df1['cup_size'] = le.fit_transform(df1['cup_size'])\n",
        "df1['cup_size'].value_counts()"
      ],
      "id": "eXNyOT9D4R5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stxbsv-z4R5f"
      },
      "source": [
        "- From cup_size, we will derive the bust size.\n",
        "- From https://www.macys.com/p/bra-fit-guide/bra-size-fit-faq/, we know that:\n",
        "    - bust_size - band_size = cup_size\n",
        "- We have band and cup size, so we will have bust_size\n",
        "    - bust_size = band_size + cup_size"
      ],
      "id": "stxbsv-z4R5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9z63ObyJ4R5f"
      },
      "outputs": [],
      "source": [
        "df1['bust_size'] = df1['band_size'] + df1['cup_size']"
      ],
      "id": "9z63ObyJ4R5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP32VET94R5f"
      },
      "outputs": [],
      "source": [
        "df1['bust_size'].describe()"
      ],
      "id": "ZP32VET94R5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tHuvgO94R5g"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['cup_size'],inplace=True)"
      ],
      "id": "4tHuvgO94R5g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRAWKDHH4R5g"
      },
      "source": [
        "- Now, I think that for a woman her bust and band size must have correlation.\n",
        "- Let's check."
      ],
      "id": "iRAWKDHH4R5g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijf_ErCZ4R5g"
      },
      "outputs": [],
      "source": [
        "print(f\"The correlation between band and bust size is: \")\n",
        "print(np.corrcoef(df1['band_size'], df1['bust_size']))"
      ],
      "id": "Ijf_ErCZ4R5g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byJMhCsQ4R5g"
      },
      "source": [
        "- This is a very high correlation."
      ],
      "id": "byJMhCsQ4R5g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou2GNugj4R5g"
      },
      "outputs": [],
      "source": [
        "sns.regplot(data=df1,x='band_size',y='bust_size')"
      ],
      "id": "ou2GNugj4R5g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe_cZQcz4R5h"
      },
      "source": [
        "- A very interesting plot indeed.\n",
        "- As expected, for one value of band_size, there are multiple values of bust_size.\n",
        "- To preserve the information contained in both these variables, and do away with correlation, I am going to keep their dot product and save in a new feature called 'chest'.\n"
      ],
      "id": "xe_cZQcz4R5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG0Oq48u4R5h"
      },
      "outputs": [],
      "source": [
        "df1['chest'] = df1['bust_size'] * df1['band_size']\n",
        "df1.drop(columns=['bust_size','band_size'],inplace=True)"
      ],
      "id": "HG0Oq48u4R5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "TOAA4gLn4R5h"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ],
      "id": "TOAA4gLn4R5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wa8LyRs4R5h"
      },
      "outputs": [],
      "source": [
        "df1['chest'].describe()"
      ],
      "id": "3wa8LyRs4R5h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETo3ZGgh4R5i"
      },
      "source": [
        "### 'fit' and 'rating'"
      ],
      "id": "ETo3ZGgh4R5i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4CykQef4R5i"
      },
      "source": [
        "- fit and rating appear to be connected to each other. A customer is expected to rate a product highly if it fits her well. So, from a practical perspective there is definitely a connection.\n",
        "- let us draw a countplot of rating with fit as hue to visualize the connection"
      ],
      "id": "r4CykQef4R5i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScqXCPb84R5i"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df1,x='rating',hue='fit')\n",
        "plt.show()"
      ],
      "id": "ScqXCPb84R5i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd_XvWB4R5i"
      },
      "source": [
        "- As is clear from the above graph, \n",
        "  - customers have rated a product highly if they fit them well \n",
        "  - if the product is small or large, then also, a lot of customers have rated the product highly. \n",
        "  - The count of people rating a product as 1,2 and 3 is significantly small as compared to the count of people rating the product as 4 and 5.  \n",
        "  - So, customers are very specific about their rating if the product is small or large- that is they have rated them lowly, to the extent that one can, from statistical perspective, merge large and small into one subset.\n",
        "  - Let's verify"
      ],
      "id": "3cd_XvWB4R5i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3-Juu_54R5i"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df1.groupby(\"fit\")[\"rating\"].value_counts())"
      ],
      "id": "_3-Juu_54R5i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDiG8wG74R5j"
      },
      "source": [
        "- There is little statistical difference between the distribution of customers rating a produst as small or large. \n",
        "- The count of customers rating a product is 1,2,3,4 or 5 are almost same for the classes of small and large. So, we can merge into one as not fit. \n",
        "- So I am converting the feature fit into a binary one. \n"
      ],
      "id": "HDiG8wG74R5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVyA7RZa4R5j"
      },
      "outputs": [],
      "source": [
        "df1[\"fit\"] = np.where((df1[\"fit\"] == 'fit'), 1, 0)"
      ],
      "id": "HVyA7RZa4R5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RO1fD1U4R5j"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=df1,x='rating',hue='fit')\n",
        "plt.show()"
      ],
      "id": "3RO1fD1U4R5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KE05oB_G4R5j"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(df1.groupby(\"rating\")[\"fit\"].value_counts())"
      ],
      "id": "KE05oB_G4R5j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox1nAuO54R5j"
      },
      "source": [
        "- The trend continues. It the product fits, the customers are going to rate it highly as in, either 4 or five\n",
        "- The informations contained in the feature 'rating' also contains within itself the necessary information about the customer's choice.\n",
        "- We do not really need the 'fit' feature in the context of customer segmentation.\n",
        "- But to be sure, we will perform a Kruskal Wallis test (as rating is not normally distributed)"
      ],
      "id": "Ox1nAuO54R5j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsrr75P94R5k"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kruskal"
      ],
      "id": "bsrr75P94R5k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SrkMw124R5k"
      },
      "outputs": [],
      "source": [
        "stats.kruskal(df1['rating'][df1['fit'] == 0],\\\n",
        "               df1['rating'][df1['fit'] == 1])"
      ],
      "id": "4SrkMw124R5k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDTIpXzG4R5k"
      },
      "source": [
        "- A very high value of F statistic and low value of p shows that we have sufficient proof to reject the null hypothesis that the median rating is same for both groups of fit- YES and NO. \n",
        "- We can claim that different rating values mean that fit did not happen.\n",
        "- So the two features - 'rating' and 'fit' are dependent on each other, something we knew intuitively. A person won't rate somethiing highly unless they are sure if the item fits.\n",
        "- So, I can get rid of any one of them. Or keep both of them by saving their dot product in a new vector.\n",
        "- While multiplying the vectors, I will make sure to increment fit values by 1."
      ],
      "id": "IDTIpXzG4R5k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi6bGD2O4R5k"
      },
      "outputs": [],
      "source": [
        "df1['response'] = df1['rating'] * (df1['fit']+1)"
      ],
      "id": "Qi6bGD2O4R5k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5g679H24R5k"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['rating','fit'],inplace=True)"
      ],
      "id": "i5g679H24R5k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjddTqNu4R5k"
      },
      "source": [
        "### rented_for"
      ],
      "id": "yjddTqNu4R5k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "-gMA_uUM4R5l"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ],
      "id": "-gMA_uUM4R5l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsLSlCgh4R5l"
      },
      "source": [
        "- There are 9 classes of this feature, which is a rather huge number. \n",
        "- party and party: cocktail clearly belong to only one class- 'party'. Let's merge them"
      ],
      "id": "TsLSlCgh4R5l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCMwyZg74R5l"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='party: cocktail','rented_for'] = 'party'"
      ],
      "id": "tCMwyZg74R5l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmNuQJwI4R5l"
      },
      "source": [
        "- If a customer (female, let's not forget) goes for a date and chooses 'expensive' rented clothes, she is going to wear similar dress for the ocassions of party. To clarify, I mean to argue that the occasion of party and date can be considered as same, if we focus only on the dress the woman chooses to wear.\n",
        "- So I am reclassifying all the values with occasion as date int0 party."
      ],
      "id": "LmNuQJwI4R5l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbHfiJAk4R5l"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='date','rented_for'] = 'party'"
      ],
      "id": "GbHfiJAk4R5l"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr1L18I_4R5l"
      },
      "source": [
        "- Now, work is basically a formal affair.\n",
        "- Someone may choose to go to work in simple tee and jeans, but they won't hire is specially from a company.\n",
        "- So as the customer of renttherunway, a woman is buying for 'formal affair' if she is buying for work. \n",
        "- So I am reclassifying all the 'work' values as formal affair. \n",
        "- I am also renaming 'formal affair' as 'formal'."
      ],
      "id": "Jr1L18I_4R5l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW4LNGP_4R5m"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].replace({'formal affair':'formal'},inplace=True)"
      ],
      "id": "oW4LNGP_4R5m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OosiM6xn4R5m"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='work','rented_for'] = 'formal'"
      ],
      "id": "OosiM6xn4R5m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "eQw_Q1mg4R5m"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ],
      "id": "eQw_Q1mg4R5m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXA1lmKn4R5m"
      },
      "outputs": [],
      "source": [
        "df1.loc[df1.rented_for=='vacation','rented_for'] = 'everyday'\n",
        "df1.loc[df1.rented_for=='other','rented_for'] = 'everyday'"
      ],
      "id": "nXA1lmKn4R5m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCYhyfH4R5m"
      },
      "source": [
        "- Now, I have also categorized 'everyday','other', and 'vacation' into one. \n",
        "- The reason has more to do with statistics than real-life reasoning. \n",
        "- But of course, I have to say that when it comes to dresses, people tend to use similar types of clothes for everyday and vacation - comfortable. \n",
        "- The counts for these classes is also low, signifying a smaller cluster. I would like to see them together. I also want to reduce the sparsity of the data.\n",
        "- The idea is to prepare data for the centroid based algorithms. If I had decided to go for DBSCAN or some other neural based clustering approach, I might have tackled the dataset in a different way."
      ],
      "id": "5eCYhyfH4R5m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro6gwN9W4R5m"
      },
      "outputs": [],
      "source": [
        "df1['rented_for'].value_counts()"
      ],
      "id": "ro6gwN9W4R5m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jb0QaoE4R5n"
      },
      "source": [
        "### category"
      ],
      "id": "7Jb0QaoE4R5n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2vE7EFT4R5n"
      },
      "source": [
        "- Let's analyze 'category' feature"
      ],
      "id": "Q2vE7EFT4R5n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "aY3t2wMn4R5n"
      },
      "outputs": [],
      "source": [
        "df1['category'].value_counts()"
      ],
      "id": "aY3t2wMn4R5n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-PS66vY4R5n"
      },
      "source": [
        "- There are 68 classes of category of the item that the customers have the option to buy, considering they have exhausted all of them in this huge dataset.\n",
        "- The range of counts of individual items is significant,\n",
        "- This is going to create problems for the algorithms when they choose to cluster the data, and more so when we are limited ourselves to centroid based algorithms like KMeans and Agglomerative.\n",
        "- A huge percentage of them are 'dress'."
      ],
      "id": "i-PS66vY4R5n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIRyykeL4R5n"
      },
      "outputs": [],
      "source": [
        "dress_percent = len(df1.loc[df1['category']=='dress'])/len(df1.category)*100\n",
        "print(f'The percentage of dress bought by customers is {dress_percent}')"
      ],
      "id": "sIRyykeL4R5n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWoDNRb_4R5o"
      },
      "source": [
        "- In fact, most of the people either rent dress or gown or sheath.\n",
        "    - dress         92560\n",
        "    - gown          44160\n",
        "    - sheath        19227\n",
        "- In total 155947 items belong to either of the above three categories, which is around 3/4th of the total count of transactions. \n",
        "- And it makes sense, as most of the people come here to rent for occasions like wedding or party.\n",
        "- Now, I will club of the classes into one, and call it as 'others'."
      ],
      "id": "JWoDNRb_4R5o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8sHQa914R5o"
      },
      "outputs": [],
      "source": [
        "df1['category'] = [x if x in {'dress','gown','sheath'} else 'others' for x in df1['category']]"
      ],
      "id": "U8sHQa914R5o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uB9Nc5cc4R5o"
      },
      "outputs": [],
      "source": [
        "df1['category'].value_counts()"
      ],
      "id": "uB9Nc5cc4R5o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g237Rqq4R5o"
      },
      "source": [
        "### body _type and size"
      ],
      "id": "-g237Rqq4R5o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL0OUVtT4R5o"
      },
      "source": [
        "- let's first see what are the various values in the column"
      ],
      "id": "DL0OUVtT4R5o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OkdSeJh4R5o"
      },
      "outputs": [],
      "source": [
        "df1['body_type'].value_counts()"
      ],
      "id": "5OkdSeJh4R5o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLIrqOl4R5p"
      },
      "source": [
        "- body_type has the body type of customer\n",
        "- it is filled by the customer\n",
        "- And intuitively, it is easy to see that it has strong connections with the customer's weight, height, chest and size of the item they choose to rent.\n",
        "- Let's look at them a little closely."
      ],
      "id": "GXLIrqOl4R5p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Wi3lO4EV4R5p"
      },
      "outputs": [],
      "source": [
        "df1[['body_type','size','weight','height','chest']].sample(50)"
      ],
      "id": "Wi3lO4EV4R5p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCA_Ssd-4R5p"
      },
      "source": [
        "- First of all, 'size' is quite dubious. We have no idea what measure has been used, as discussed earlier. This feature is either the beyond my scope (as my business understanding is limited) or there is something seriously fishy about the values in this column- something I strongly believe, based on my reading of the subject.\n",
        "- But I do see a sort of correlation it has with weight. Let's check."
      ],
      "id": "HCA_Ssd-4R5p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "87sT1iIL4R5p"
      },
      "outputs": [],
      "source": [
        "sns.regplot(data=df1,x='weight',y='size')\n",
        "plt.show()"
      ],
      "id": "87sT1iIL4R5p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx8ZnKH84R5p"
      },
      "source": [
        "- There also seems be a very high correlation between weight and size, which can be substantiated with a corr plot"
      ],
      "id": "wx8ZnKH84R5p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "uRC505ia4R5p"
      },
      "outputs": [],
      "source": [
        "df1.corr()"
      ],
      "id": "uRC505ia4R5p"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auyKNbRo4R5q"
      },
      "source": [
        "- So I am going to drop it. "
      ],
      "id": "auyKNbRo4R5q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT8xmglw4R5q"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['size'],inplace=True)"
      ],
      "id": "bT8xmglw4R5q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXQW9qj94R5q"
      },
      "source": [
        "- Now, looking at the feature classes of body_type, there seem to be many interesting names. \n",
        "- That is for marketting purpose. Or perhaps, the data analysis department of renttherunway has a definition of these terms.\n",
        "- As for us, who are trying to feed them into ML algorithms, we cannot be sure about everything except for the relationship body_type PREMPTIVELY has with features like 'height','weight','band_size' and 'cup_size'.\n",
        "- I can so far as to claim that body_type is a polynomial combination of the other features. And this is not just intuition. I see it from the 50 samples. \n",
        "- But, and here is my key argument- these values have been fed by the customers, many of whom were not aware of the temrs either. \n",
        "    - some were in a hurry, so they chose anything.\n",
        "    - some wanted to show off and lied.\n",
        "    - some were confused between apple and pear. \n",
        "- That makes 'body_type' a subjective variable. It has more to do customers' self perception that the reality.\n",
        "- But their BIOLOGICAL features are a better source of their body_type. They are more OBJECTIVE.\n",
        "- So, for the sake of objectivity, I am going to drop this feature. \n"
      ],
      "id": "wXQW9qj94R5q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dewRwv-84R5q"
      },
      "outputs": [],
      "source": [
        "df1.drop(columns=['body_type'],inplace=True)"
      ],
      "id": "dewRwv-84R5q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vN4AWRW4R5q"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ],
      "id": "0vN4AWRW4R5q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4aj5k7m4R5q"
      },
      "outputs": [],
      "source": [
        "df2 = df1"
      ],
      "id": "d4aj5k7m4R5q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19ozc4C14R5r"
      },
      "outputs": [],
      "source": [
        "df1.to_csv('cleaned2_renttherunway.csv')"
      ],
      "id": "19ozc4C14R5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c448b1f7",
      "metadata": {
        "id": "c448b1f7"
      },
      "outputs": [],
      "source": [
        "df2.sample(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKWIeFL4R5r"
      },
      "source": [
        "# 3. Data Preparation for model building"
      ],
      "id": "wtKWIeFL4R5r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCFDtkon4R5r"
      },
      "source": [
        "- But before that, we have to \n",
        "  - one hot encode, and\n",
        "  - standardize the data"
      ],
      "id": "UCFDtkon4R5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvtArRe-4R5r"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()"
      ],
      "id": "XvtArRe-4R5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D__oh0d4R5r"
      },
      "outputs": [],
      "source": [
        "to_be_encoded = df2.select_dtypes(include='object').columns"
      ],
      "id": "9D__oh0d4R5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsfDLocR4R5s"
      },
      "outputs": [],
      "source": [
        "for feature in to_be_encoded:\n",
        "  df2[feature] = le.fit_transform(df2[feature])"
      ],
      "id": "OsfDLocR4R5s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5B-I6na4R5s"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ],
      "id": "u5B-I6na4R5s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v769YZGE4R5s"
      },
      "outputs": [],
      "source": [
        "## Standardization\n",
        "scaled_features = StandardScaler().fit_transform(df2.values)\n",
        "df2 = pd.DataFrame(scaled_features, index=df2.index, columns=df2.columns)"
      ],
      "id": "v769YZGE4R5s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUKoP5zA4R5s"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ],
      "id": "VUKoP5zA4R5s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mi8v4Vg4R5t"
      },
      "outputs": [],
      "source": [
        "df2.corr()"
      ],
      "id": "4Mi8v4Vg4R5t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8kwoVJ_4R5t"
      },
      "outputs": [],
      "source": [
        "df2.to_csv('prepared_renttherunway.csv')"
      ],
      "id": "n8kwoVJ_4R5t"
    },
    {
      "cell_type": "markdown",
      "id": "Jntox961vPAr",
      "metadata": {
        "id": "Jntox961vPAr"
      },
      "source": [
        "On df2, I will apply PCA, and df3 will be the one I will put into clustering algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IsXj5fIKtCU3",
      "metadata": {
        "id": "IsXj5fIKtCU3"
      },
      "source": [
        "# 4. Principal Component Analysis and Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85605e04",
      "metadata": {
        "id": "85605e04"
      },
      "source": [
        "## Dimensionality Reduction with Principal Component Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42172ec3",
      "metadata": {
        "id": "42172ec3"
      },
      "outputs": [],
      "source": [
        "## Calculating covariance matrix\n",
        "cov_matrix = np.cov(df3.T)\n",
        "print('Covariance matrix','\\n',cov_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595b3049",
      "metadata": {
        "id": "595b3049"
      },
      "outputs": [],
      "source": [
        "## Calculating eigen values and eigen vectors\n",
        "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
        "print('Eigen vectors:','\\n',eig_vecs)\n",
        "print('\\n')\n",
        "print('Eigen values:','\\n',eig_vals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bfa766",
      "metadata": {
        "id": "34bfa766"
      },
      "outputs": [],
      "source": [
        "total = sum(eig_vals)\n",
        "var_exp = [ (i/total)*100  for i in sorted(eig_vals,reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "print('Variance Explained: ',var_exp)\n",
        "print('Cummulative Variance Explained: ',cum_var_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38a9509",
      "metadata": {
        "id": "f38a9509"
      },
      "outputs": [],
      "source": [
        "plt.bar(range(7),var_exp, align='center',color='lightgreen',edgecolor='black',label='Indiviual Explained Varinace')\n",
        "plt.step(range(7), cum_var_exp, where='mid',color='red',label='Cummulative explained Variance')\n",
        "plt.legend(loc = 'best')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea02c1c7",
      "metadata": {
        "id": "ea02c1c7"
      },
      "source": [
        "- We can see that approximately 93.5% of variance is explained by the first 6 features.\n",
        "- so, we can choose the optimal number of principal components as 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159b5aa6",
      "metadata": {
        "id": "159b5aa6"
      },
      "outputs": [],
      "source": [
        "pca=PCA(n_components = 6)\n",
        "pca.fit(df3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b86278",
      "metadata": {
        "id": "63b86278"
      },
      "outputs": [],
      "source": [
        "#transformed dataset after PCA is df4.\n",
        "df4 = pca.transform(df3)\n",
        "df4 = pd.DataFrame(df4,columns=['PC1','PC2','PC3','PC4','PC5','PC6'])\n",
        "df4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83884044",
      "metadata": {
        "id": "83884044"
      },
      "source": [
        "## K-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1cd8da",
      "metadata": {
        "id": "9a1cd8da"
      },
      "outputs": [],
      "source": [
        "cluster_range = range(1,12)\n",
        "cluster_errors = []\n",
        "\n",
        "for num_clusters in cluster_range:\n",
        "    clusters = KMeans(num_clusters, init='k-means++', n_init=20, random_state=42)\n",
        "    clusters.fit(df4)\n",
        "    labels = clusters.labels_\n",
        "    centroids = clusters.cluster_centers_\n",
        "    cluster_errors.append(clusters.inertia_)\n",
        "clusters_df = pd.DataFrame({'num_clusters':cluster_range, \n",
        "                           'cluster_errors':cluster_errors})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b969557",
      "metadata": {
        "id": "3b969557"
      },
      "outputs": [],
      "source": [
        "## Elbow method\n",
        "plt.figure(figsize=[10,5])\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters using PCA')\n",
        "plt.plot(clusters_df['num_clusters'],clusters_df['cluster_errors'],marker='o',color='b')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3fbd78",
      "metadata": {
        "id": "8d3fbd78"
      },
      "source": [
        "- From the Elbow plot, we can see that at K= 5 or 6, the interia starts to drop significantly. \n",
        "- We also calculate Silhoutte Scores for various possible clusters. We find that 5 gives us the best value. So, we will go ahead with 5 clusters.\n",
        "- The clusters are labeled as 0,1,2,3,4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718569ae",
      "metadata": {
        "id": "718569ae"
      },
      "outputs": [],
      "source": [
        "## Fit the KMeans clustering model using the obtained optimal K\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', n_init=20, random_state=42)\n",
        "kmeans.fit(df4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fb2524",
      "metadata": {
        "id": "a5fb2524"
      },
      "outputs": [],
      "source": [
        "## Creating a new dataframe only for labels and converting it into categorical variables.\n",
        "df_labels = pd.DataFrame(kmeans.labels_, columns=list(['Labels']))\n",
        "df_labels['Labels'] = df_labels['Labels'].astype('category')\n",
        "## joining the label dataframe with unscaled initial dataframe.(df)\n",
        "df_kmeans = df3.join(df_labels)\n",
        "df_kmeans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9e57c0",
      "metadata": {
        "id": "2d9e57c0"
      },
      "outputs": [],
      "source": [
        "df_kmeans['Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58026f8a",
      "metadata": {
        "id": "58026f8a"
      },
      "source": [
        "### Silhoutte Score for validating the best optimal number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc5c61a",
      "metadata": {
        "id": "0bc5c61a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6a056c",
      "metadata": {
        "id": "ec6a056c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# I limit my study of clusters from 5 to 7\n",
        "\n",
        "kmeans_score = []\n",
        "\n",
        "for i in range(4,9):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', n_init=10, random_state=42)\n",
        "    kmeans = kmeans.fit(df4)\n",
        "    labels = kmeans.predict(df4)\n",
        "    print(i,'   ',silhouette_score(df4,labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de814ba",
      "metadata": {
        "id": "9de814ba"
      },
      "source": [
        " - From above, we can observe that for 5 and 6 clusters the silhoutte score is highest, we can choose optimal clusters as 5 or 6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69e6e801",
      "metadata": {
        "id": "69e6e801"
      },
      "source": [
        "## Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0adda1",
      "metadata": {
        "id": "ae0adda1"
      },
      "outputs": [],
      "source": [
        "df4.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27268f7f",
      "metadata": {
        "id": "27268f7f"
      },
      "outputs": [],
      "source": [
        "df4ac = df4.sample(frac=0.50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5f5f129",
      "metadata": {
        "id": "e5f5f129"
      },
      "outputs": [],
      "source": [
        "df4ac.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b409012",
      "metadata": {
        "id": "0b409012"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[18,7])\n",
        "merg = linkage(df4ac, method='ward')\n",
        "dendrogram(merg, leaf_rotation=90,)\n",
        "plt.xlabel('Datapoints')\n",
        "plt.ylabel('Euclidean distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94321917",
      "metadata": {
        "id": "94321917"
      },
      "outputs": [],
      "source": [
        "## Building hierarchical clustering model using the optimal clusters as 4\n",
        "hie_cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean',\n",
        "                                     linkage='ward')\n",
        "hie_cluster_model = hie_cluster.fit(data_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a57e5d1",
      "metadata": {
        "id": "3a57e5d1"
      },
      "outputs": [],
      "source": [
        "## Creating a dataframe of the labels\n",
        "df_label1 = pd.DataFrame(hie_cluster_model.labels_,columns=['Labels'])\n",
        "df_label1.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900f379a",
      "metadata": {
        "id": "900f379a"
      },
      "outputs": [],
      "source": [
        "## joining the label dataframe with unscaled initial dataframe.(df)\n",
        "\n",
        "df_hier = df1.join(df_label1)\n",
        "df_hier.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d024393",
      "metadata": {
        "id": "0d024393"
      },
      "source": [
        "### Q18. Compute Silhoutte Score for validating the best optimal number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f996fe28",
      "metadata": {
        "id": "f996fe28"
      },
      "outputs": [],
      "source": [
        "for i in range(2,15):\n",
        "    hier = AgglomerativeClustering(n_clusters=i)\n",
        "    hier = hier.fit(data_pca)\n",
        "    labels = hier.fit_predict(data_pca)\n",
        "    print(i,silhouette_score(data_pca,labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa15cc93",
      "metadata": {
        "id": "fa15cc93"
      },
      "source": [
        "- From above, we can observe that the silhouette score is highest for 6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a0370d",
      "metadata": {
        "id": "88a0370d"
      },
      "source": [
        "## Conclustion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8e4b64",
      "metadata": {
        "id": "be8e4b64"
      },
      "source": [
        "- In this case study, we have attempted to cluster adult census dataset using K-means and agglomerative clustering and we also reduced the dimensionality of the dataset using PCA.\n",
        "- We came up with 6 clusters using K-means and 4 classes using agglomerative clustering.\n",
        "- Although selection of the clusters can be revised using Silhoutte score but for a general introductory part it is okay to visualize the plot (either elbow graph or dendrograms) and come up with a particular clusters size.\n",
        "- Further, we can also do the cluster analysis by doing bivariate analysis between cluster labels and different features and understand the characteristics of different groups."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Customer Segmentation on RentTheRunWay dataset with Centroid based clustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}